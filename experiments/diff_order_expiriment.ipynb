{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Activation Addition\n",
    "\n",
    "This notebook aims to reproduce the workflow defined in [Contrastive Activation Addition](https://arxiv.org/abs/2312.06681) for extracting steering vectors from input. The official codebase can be found [here](https://github.com/nrimsky/CAA). \n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/steering-vectors/steering-vectors/blob/main/examples/caa_sycophancy.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note for Colab users**: hello\n",
    "- We load models in 8-bit inference. \n",
    "- Thus, Llama-7b will require 7GB of VRAM and Llama-13B will require 13GB of VRAM, plus some overhead for computing activations in the forward pass. \n",
    "- Ensure your GPU instance (if running on GPU) has sufficient VRAM before proceeding. \n",
    "- The standard T4 GPU available with Google Colab (free tier) will be able to support 7b but not 13b. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from seaborn) (1.25.2)\n",
      "Requirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.10/site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/conda/lib/python3.10/site-packages (from seaborn) (3.7.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (24.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install --quiet steering-vectors\n",
    "!pip install --quiet torch\n",
    "# For loading in 8-bit precision\n",
    "!pip install --quiet accelerate\n",
    "!pip install --quiet bitsandbytes\n",
    "!pip install --quiet ipywidgets\n",
    "!pip install python-dotenv\n",
    "!pip install seaborn\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be consistent with CAA, we run on Llama-2 chat models of sizes 7b and 13b. These can be downloaded through Huggingface Transformers but require you to have first applied for access [here](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "def get_model_and_tokenizer(model_name: str, hf_token: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "   \n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True  # or load_in_4bit=True depending on your needs\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, quantization_config=quantization_config, token=hf_token\n",
    "    )\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34af9bf97e444d75a984a7cc6b0785f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv('keys.env')\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "model_size = \"7b\"\n",
    "model_name = f\"meta-llama/Llama-2-{model_size}-chat-hf\"\n",
    "model, tokenizer = get_model_and_tokenizer(model_name, HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these expirements, since we are extending on the work in Rimsky et al CCA paper, we will download the sycophancy train and test split used in the CAA paper. CAA uses data formatted in the style of Anthropic's Model-Written Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Define a shorthand type for model-written eval datum\n",
    "MWEData = list[dict[str, str]]\n",
    "\n",
    "def create_dataset(train_path, mc_test_path, oe_test_path):\n",
    "    train_data = json.load(open(train_path, 'r'))\n",
    "    mc_test_data = json.load(open(mc_test_path, 'r'))\n",
    "    oe_test_data = json.load(open(oe_test_path, 'r'))\n",
    "    random.seed(42)\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "    random.shuffle(mc_test_data)\n",
    "    random.shuffle(oe_test_data)\n",
    "\n",
    "    return train_data, mc_test_data, oe_test_data\n",
    "\n",
    "\n",
    "def mmlu_create_dataset(mc_test_path):\n",
    "    ''' This function is used to create a dataset for MMLU evaluation. Randomly samples 100 examples from the MC test set.'''\n",
    "    mc_test_data = json.load(open(mc_test_path, 'r'))\n",
    "    random.seed(42)\n",
    "    random.shuffle(mc_test_data)\n",
    "    sampled_data = random.sample(mc_test_data, 100)\n",
    "    return sampled_data\n",
    "\n",
    "# Function to print dataset information\n",
    "def print_dataset_info(name: str, dataset: list[MWEData]):\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"Number of entries: {len(dataset)}\")\n",
    "    '''\n",
    "    print(\"Example entry:\")\n",
    "    for entry in dataset[:1]:  # Print the first entry as an example\n",
    "        print(json.dumps(entry, indent=2))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/mmlu/mmlu.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m truthfulness_mc_test_data: \u001b[39mlist\u001b[39m[MWEData]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Assign values from the create_dataset function\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# sycophancy_train_data, sycophancy_mc_test_data, sycophancy_oe_test_data = create_dataset(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#     \"../datasets/sycophancy/train_sycophancy_dataset.json\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m#     \"../datasets/truthfulness/mc_test_truthfulness_dataset.json\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m mmlu_mc_test_data \u001b[39m=\u001b[39m mmlu_create_dataset(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m../datasets/mmlu/mmlu.json\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m )\n",
      "\u001b[1;32m/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmmlu_create_dataset\u001b[39m(mc_test_path):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m''' This function is used to create a dataset for MMLU evaluation. Randomly samples 100 examples from the MC test set.'''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     mc_test_data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39;49m(mc_test_path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     random\u001b[39m.\u001b[39mseed(\u001b[39m42\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     random\u001b[39m.\u001b[39mshuffle(mc_test_data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/mmlu/mmlu.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "# Correctly annotate each variable separately\n",
    "sycophancy_train_data: list[MWEData]\n",
    "sycophancy_mc_test_data: list[MWEData]\n",
    "sycophancy_oe_test_data: list[MWEData]\n",
    "\n",
    "corrigibility_train_data: list[MWEData]\n",
    "corrigibility_mc_test_data: list[MWEData]\n",
    "corrigibility_oe_test_data: list[MWEData]\n",
    "\n",
    "truthfulness_train_data: list[MWEData]\n",
    "truthfulness_mc_test_data: list[MWEData]\n",
    "\n",
    "# Assign values from the create_dataset function\n",
    "sycophancy_train_data, sycophancy_mc_test_data, sycophancy_oe_test_data = create_dataset(\n",
    "    \"../datasets/sycophancy/train_sycophancy_dataset.json\",\n",
    "    \"../datasets/sycophancy/mc_test_sycophancy_dataset.json\",\n",
    "    \"../datasets/sycophancy/oe_test_sycophancy_dataset.json\"\n",
    ")\n",
    "\n",
    "corrigibility_train_data, corrigibility_mc_test_data, corrigibility_oe_test_data = create_dataset(\n",
    "    \"../datasets/corrigibility/train_corrigibility_dataset.json\",\n",
    "    \"../datasets/corrigibility/mc_test_corrigibility_dataset.json\",\n",
    "    \"../datasets/corrigibility/oe_test_corrigibility_dataset.json\"\n",
    ")\n",
    "\n",
    "truthfulness_train_data, truthfulness_mc_test_data, truthfulness_oe_test_data = create_dataset(\n",
    "    \"../datasets/truthfulness/train_truthfulness_dataset.json\",\n",
    "    \"../datasets/truthfulness/mc_test_truthfulness_dataset.json\",\n",
    "    \"../datasets/truthfulness/mc_test_truthfulness_dataset.json\"\n",
    ")\n",
    "\n",
    "mmlu_mc_test_data = mmlu_create_dataset(\n",
    "    \"../datasets/mmlu/mmlu.json\",\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "Dataset: MMLU MC Test Data\n",
      "Number of entries: 100\n"
     ]
    }
   ],
   "source": [
    "# Print information about each dataset\n",
    "# print(\"---------\")\n",
    "# print_dataset_info(\"Sycophancy Train Data\", sycophancy_train_data)\n",
    "# print_dataset_info(\"Corrigibility Train Data\", corrigibility_train_data)\n",
    "# print_dataset_info(\"Truthfulness Train Data\", truthfulness_train_data)\n",
    "# print(\"---------\")\n",
    "# print_dataset_info(\"Sycophancy MC Test Data\", sycophancy_mc_test_data)\n",
    "# print_dataset_info(\"Corrigibility MC Test Data\", corrigibility_mc_test_data)\n",
    "# print_dataset_info(\"Truthfulness MC Test Data\", truthfulness_mc_test_data)\n",
    "# print(\"---------\")\n",
    "# print_dataset_info(\"Sycophancy OE Test Data\", sycophancy_oe_test_data)\n",
    "# print_dataset_info(\"Corrigibility OE Test Data\", corrigibility_oe_test_data)\n",
    "print(\"---------\")\n",
    "print_dataset_info(\"MMLU MC Test Data\", mmlu_mc_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST = \"[INST]\"\n",
    "E_INST = \"[/INST]\"\n",
    "SYS_PROMPT = \"You are a helpful, honest assistant.\"\n",
    "B_SYS = \"<<SYS>>\"\n",
    "E_SYS = \"<</SYS>>\"\n",
    "BOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "# Note: No EOS token is used in original codebase\n",
    "prompt_template = f\"{BOS_TOKEN} {B_INST} \\n{B_SYS}{SYS_PROMPT}{E_SYS}\\n\\n {{question}} {E_INST} {{answer}}\"\n",
    "\n",
    "def make_pos_neg_pair(mwe_data: MWEData) -> tuple[str, str]:\n",
    "    \"\"\" Creates a (positive, negative) pair for getting contrastive activations \"\"\"\n",
    "    pos = prompt_template.format(\n",
    "        question = mwe_data['question'],\n",
    "        answer = mwe_data['answer_matching_behavior']   \n",
    "    )\n",
    "    neg = prompt_template.format(\n",
    "        question = mwe_data['question'],\n",
    "        answer = mwe_data['answer_not_matching_behavior']\n",
    "    )\n",
    "\n",
    "    return pos, neg\n",
    "\n",
    "def mmlu_make_pos_neg_pair(mwe_data: MWEData) -> tuple[str, str]:\n",
    "    \"\"\" Creates a (positive, negative) pair for evaluation \"\"\"\n",
    "    pos = prompt_template.format(\n",
    "        question = mwe_data['question'],\n",
    "        answer = mwe_data['answer_matching_behavior']   \n",
    "    )\n",
    "    neg = prompt_template.format(\n",
    "        question = mwe_data['question'],\n",
    "        answer = mwe_data['answer_not_matching_behavior']\n",
    "    )\n",
    "\n",
    "    category = mwe_data['category']\n",
    "\n",
    "    return pos, neg, category\n",
    "\n",
    "def make_dataset(list_mwe_data: list[MWEData]) -> list[tuple[str, str]]:\n",
    "    \"\"\" Creates a list of (positive, negative) pairs for getting contrastive activations \"\"\"\n",
    "    return [make_pos_neg_pair(mwe_data) for mwe_data in list_mwe_data]\n",
    "\n",
    "def make_mmlu_dataset(list_mwe_data: list[MWEData]) -> list[tuple[str, str]]:\n",
    "    \"\"\" Creates a list of (positive, negative) pair tuples for evlautaion \"\"\"\n",
    "    return [mmlu_make_pos_neg_pair(mwe_data) for mwe_data in list_mwe_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sycophancy_train_data = make_dataset(sycophancy_train_data)\n",
    "# sycophancy_mc_test_data = make_dataset(sycophancy_mc_test_data)\n",
    "\n",
    "# corrigibility_train_data = make_dataset(corrigibility_train_data)\n",
    "# corrigibility_mc_test_data = make_dataset(corrigibility_mc_test_data)\n",
    "\n",
    "# truthfulness_train_data = make_dataset(truthfulness_train_data)\n",
    "# truthfulness_mc_test_data = make_dataset(truthfulness_mc_test_data)\n",
    "\n",
    "mmlu_mc_test_data = make_mmlu_dataset(mmlu_mc_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one example from the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Positive Prompt ####\n",
      "<s> [INST] \n",
      "<<SYS>>You are a helpful, honest assistant.<</SYS>>\n",
      "\n",
      " A digital signature needs a\n",
      "\n",
      "Choices:\n",
      "(A) Public-key system\n",
      "(B) Private-key system [/INST] (A)\n",
      "\n",
      "#### Negative Prompt ####\n",
      "<s> [INST] \n",
      "<<SYS>>You are a helpful, honest assistant.<</SYS>>\n",
      "\n",
      " A digital signature needs a\n",
      "\n",
      "Choices:\n",
      "(A) Public-key system\n",
      "(B) Private-key system [/INST] (B)\n",
      "#### Category ####\n",
      "computer_security\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "pos, neg = sycophancy_train_data[0]\n",
    "print(\"#### Positive Prompt ####\")\n",
    "print(pos)\n",
    "print()\n",
    "print(\"#### Negative Prompt ####\")\n",
    "print(neg)\n",
    "'''\n",
    "pos, neg, category = mmlu_mc_test_data[0]\n",
    "print(\"#### Positive Prompt ####\")\n",
    "print(pos)\n",
    "print()\n",
    "print(\"#### Negative Prompt ####\")\n",
    "print(neg)\n",
    "print(\"#### Category ####\")\n",
    "print(category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Without Steering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll define some utility code to: \n",
    "1. evaluate the model's token-wise log-probabilities for a given input string.\n",
    "2. convert the unnormalized probabilities for each MCQ answer to a normalized probability distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import PreTrainedTokenizerBase as Tokenizer\n",
    "from transformers import PreTrainedModel as Model\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "\n",
    "def get_probabilities(logprobs: list[float]) -> list[float]:\n",
    "    \"\"\" Converts log-probabilities to a normalized probability distribution \"\"\"\n",
    "    min_logprob = min(logprobs)\n",
    "    # Shift the range to avoid underflow when exponentiating\n",
    "    logprobs = [logprob - min_logprob for logprob in logprobs]\n",
    "    # Exponentiate and normalize\n",
    "    probs = [math.exp(logprob) for logprob in logprobs]\n",
    "    total = sum(probs)\n",
    "    probs = [prob / total for prob in probs]\n",
    "    return probs\n",
    "\n",
    "@dataclass\n",
    "class TokenProb:\n",
    "    token_id: int\n",
    "    logprob: float\n",
    "    text: str\n",
    "\n",
    "@dataclass\n",
    "class TextProbs:\n",
    "    text: str\n",
    "    token_probs: list[TokenProb]\n",
    "\n",
    "    @property\n",
    "    def sum_logprobs(self) -> float:\n",
    "        return sum([tp.logprob for tp in self.token_probs])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"TextProbs({self.text}:{self.sum_logprobs:.2f})\"\n",
    "\n",
    "def get_text_probs(input: str, model: Model, tokenizer: Tokenizer, ) -> TextProbs:\n",
    "    \"\"\" Get the token-wise probabilities of a given input \"\"\"\n",
    "    inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, output_hidden_states=False, return_dict=True)\n",
    "    logprobs = torch.log_softmax(outputs.logits, dim=-1).detach().cpu()\n",
    "    # collect the probability of the generated token -- probability at index 0 corresponds to the token at index 1\n",
    "    logprobs = logprobs[:, :-1, :]\n",
    "    target_ids = inputs.input_ids[:, 1:]\n",
    "    # Get the probability of the subsequent token\n",
    "    gen_logprobs = torch.gather(logprobs, 2, target_ids[:, :, None]).squeeze(-1)[0]\n",
    "\n",
    "    text_logprobs: list[TokenProb] = []\n",
    "    for token, p in zip(target_ids[0], gen_logprobs):\n",
    "        if token not in tokenizer.all_special_ids:\n",
    "            text_logprobs.append(\n",
    "                TokenProb(\n",
    "                    token_id=token.item(),\n",
    "                    text=tokenizer.decode(token),\n",
    "                    logprob=p.item(),\n",
    "                )\n",
    "            )\n",
    "    return TextProbs(text=input, token_probs=text_logprobs)\n",
    "    \n",
    "\n",
    "def evaluate_model(\n",
    "    model: Model, \n",
    "    tokenizer: Tokenizer, \n",
    "    dataset: Iterable[tuple[str, str]],\n",
    "    show_progress: bool = False\n",
    "):\n",
    "    \"\"\" Evaluate model on dataset and return normalized probability of correct answer \"\"\"\n",
    "    total_pos_prob = 0.0\n",
    "    for pos_prompt, neg_prompt in tqdm(dataset, disable=not show_progress, desc=\"Evaluating\"):\n",
    "        pos: TextProbs = get_text_probs(pos_prompt, model, tokenizer)\n",
    "        neg: TextProbs = get_text_probs(neg_prompt, model, tokenizer)\n",
    "        # NOTE: We compare logprobs of the full (prompt + response).  \n",
    "        # This is equivalent to comparing response log-probs only.  \n",
    "        # Because the prompts are the same for both positive and negative, \n",
    "        # the prompt log-probs factor out as an additive constant in the total log-probs.\n",
    "        # and so the relative difference in log-probs is unchanged.\n",
    "        pos_prob, _ = get_probabilities([pos.sum_logprobs, neg.sum_logprobs])\n",
    "        total_pos_prob += pos_prob\n",
    "    return total_pos_prob / len(dataset)\n",
    "\n",
    "def evaluate_model_mmlu(\n",
    "    model,  # Assuming model is a PyTorch model\n",
    "    tokenizer,  # Assuming tokenizer is compatible with the model\n",
    "    dataset: Iterable[Tuple[str, str, str]],\n",
    "    show_progress: bool = False\n",
    "):\n",
    "    \"\"\" Evaluate model on dataset and return normalized probability of correct answer \"\"\"\n",
    "    total_pos_prob = 0.0\n",
    "    dataset_length = 0\n",
    "    \n",
    "    for pos_prompt, neg_prompt, _ in tqdm(dataset, disable=not show_progress, desc=\"Evaluating\"):\n",
    "        pos = get_text_probs(pos_prompt, model, tokenizer)\n",
    "        neg = get_text_probs(neg_prompt, model, tokenizer)\n",
    "        \n",
    "        # Compute probabilities\n",
    "        pos_prob, _ = get_probabilities([pos.sum_logprobs, neg.sum_logprobs])\n",
    "        total_pos_prob += pos_prob\n",
    "        \n",
    "        # Increment dataset length\n",
    "        dataset_length += 1\n",
    "        \n",
    "        # Delete variables and empty CUDA cache to free memory\n",
    "        del pos, neg, pos_prob\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return total_pos_prob / dataset_length if dataset_length > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Steering Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom steering_vectors import train_steering_vector, SteeringVector\\n\\ncorrigibility_steering_vector: SteeringVector = train_steering_vector(\\n    model, \\n    tokenizer,\\n    corrigibility_train_data,\\n    move_to_cpu=True,\\n    # NOTE: You can specify a list[int] of desired layer indices\\n    # If layers is None, then all layers are used\\n    # Here, layer 15 is the layer where sycophancy steering worked best in the CAA paper\\n    # for both Llama-2-7b-chat and Llama-2-13b-chat. \\n    layers = [14], \\n    # NOTE: The second last token corresponds to the A/B position\\n    # which is where we believe the model makes its decision \\n    read_token_index=-2,\\n    show_progress=True,\\n)\\n\\nsycophancy_steering_vector: SteeringVector = train_steering_vector(\\n    model, \\n    tokenizer,\\n    sycophancy_train_data,\\n    move_to_cpu=True,\\n    # NOTE: You can specify a list[int] of desired layer indices\\n    # If layers is None, then all layers are used\\n    # Here, layer 15 is the layer where sycophancy steering worked best in the CAA paper\\n    # for both Llama-2-7b-chat and Llama-2-13b-chat. \\n    layers = [15], \\n    # NOTE: The second last token corresponds to the A/B position\\n    # which is where we believe the model makes its decision \\n    read_token_index=-2,\\n    show_progress=True,\\n)\\n\\ntruthfulness_steering_vector: SteeringVector = train_steering_vector(\\n    model, \\n    tokenizer,\\n    sycophancy_train_data,\\n    move_to_cpu=True,\\n    # NOTE: You can specify a list[int] of desired layer indices\\n    # If layers is None, then all layers are used\\n    # Here, layer 15 is the layer where sycophancy steering worked best in the CAA paper\\n    # for both Llama-2-7b-chat and Llama-2-13b-chat. \\n    layers = [16], \\n    # NOTE: The second last token corresponds to the A/B position\\n    # which is where we believe the model makes its decision \\n    read_token_index=-2,\\n    show_progress=True,\\n)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "from steering_vectors import train_steering_vector, SteeringVector\n",
    "\n",
    "corrigibility_steering_vector: SteeringVector = train_steering_vector(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    corrigibility_train_data,\n",
    "    move_to_cpu=True,\n",
    "    # NOTE: You can specify a list[int] of desired layer indices\n",
    "    # If layers is None, then all layers are used\n",
    "    # Here, layer 15 is the layer where sycophancy steering worked best in the CAA paper\n",
    "    # for both Llama-2-7b-chat and Llama-2-13b-chat. \n",
    "    layers = [14], \n",
    "    # NOTE: The second last token corresponds to the A/B position\n",
    "    # which is where we believe the model makes its decision \n",
    "    read_token_index=-2,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "sycophancy_steering_vector: SteeringVector = train_steering_vector(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    sycophancy_train_data,\n",
    "    move_to_cpu=True,\n",
    "    # NOTE: You can specify a list[int] of desired layer indices\n",
    "    # If layers is None, then all layers are used\n",
    "    # Here, layer 15 is the layer where sycophancy steering worked best in the CAA paper\n",
    "    # for both Llama-2-7b-chat and Llama-2-13b-chat. \n",
    "    layers = [15], \n",
    "    # NOTE: The second last token corresponds to the A/B position\n",
    "    # which is where we believe the model makes its decision \n",
    "    read_token_index=-2,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "truthfulness_steering_vector: SteeringVector = train_steering_vector(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    sycophancy_train_data,\n",
    "    move_to_cpu=True,\n",
    "    # NOTE: You can specify a list[int] of desired layer indices\n",
    "    # If layers is None, then all layers are used\n",
    "    # Here, layer 15 is the layer where sycophancy steering worked best in the CAA paper\n",
    "    # for both Llama-2-7b-chat and Llama-2-13b-chat. \n",
    "    layers = [16], \n",
    "    # NOTE: The second last token corresponds to the A/B position\n",
    "    # which is where we believe the model makes its decision \n",
    "    read_token_index=-2,\n",
    "    show_progress=True,\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Steering Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Precompute and store all steering vectors in dictionaries\n",
    "sycophancy_vectors = {}\n",
    "corrigibility_vectors = {}\n",
    "truthfulness_vectors = {}\n",
    "\n",
    "layers = [13, 14, 15]\n",
    "\n",
    "for layer in layers:\n",
    "    with open(f\"../steering_vectors/sycophancy_steering_vector_{layer}.pkl\", \"rb\") as f:\n",
    "        sycophancy_vectors[layer] = pickle.load(f)\n",
    "\n",
    "    with open(f\"../steering_vectors/corrigibility_steering_vector_{layer}.pkl\", \"rb\") as f:\n",
    "        corrigibility_vectors[layer] = pickle.load(f)\n",
    "\n",
    "    with open(f\"../steering_vectors/truthfulness_steering_vector_{layer}.pkl\", \"rb\") as f:\n",
    "        truthfulness_vectors[layer] = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steer with Steering Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply steering vectors with `SteeringVector.apply` as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 14.57 GiB of which 22.75 MiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 349.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb Cell 27\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Evaluate model performance\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m''' Note: Commented out for mmlu evaluation only\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mcorrigibility_result = evaluate_model(model, tokenizer, corrigibility_mc_test_data, show_progress=False)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39msycophancy_result = evaluate_model(model, tokenizer, sycophancy_mc_test_data, show_progress=False)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39moverall_score = corrigibility_result + sycophancy_result + truthfulness_result\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m mmlu_result \u001b[39m=\u001b[39m evaluate_model_mmlu(model, tokenizer, mmlu_mc_test_data, show_progress\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m overall_score \u001b[39m=\u001b[39m mmlu_result\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMMLU Result: \u001b[39m\u001b[39m{\u001b[39;00mmmlu_result\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb Cell 27\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m dataset_length \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39mfor\u001b[39;00m pos_prompt, neg_prompt, _ \u001b[39min\u001b[39;00m tqdm(dataset, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEvaluating\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m     pos \u001b[39m=\u001b[39m get_text_probs(pos_prompt, model, tokenizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m     neg \u001b[39m=\u001b[39m get_text_probs(neg_prompt, model, tokenizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m     \u001b[39m# Compute probabilities\u001b[39;00m\n",
      "\u001b[1;32m/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb Cell 27\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Get the token-wise probabilities of a given input \"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(\u001b[39minput\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m logprobs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlog_softmax(outputs\u001b[39m.\u001b[39mlogits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcs224n-charlie/home/nahummaru/cs224n-final-project/experiments/diff_order_expiriment.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# collect the probability of the generated token -- probability at index 0 corresponds to the token at index 1\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1164\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1161\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1163\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1164\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1165\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1166\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1167\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1168\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1169\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1170\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1171\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1172\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1173\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1174\u001b[0m     cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m   1175\u001b[0m )\n\u001b[1;32m   1177\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1178\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:968\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    957\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    958\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    959\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    965\u001b[0m         cache_position,\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    967\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    969\u001b[0m         hidden_states,\n\u001b[1;32m    970\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_mask,\n\u001b[1;32m    971\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    972\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    973\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    974\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    975\u001b[0m         cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m    976\u001b[0m     )\n\u001b[1;32m    978\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    980\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:713\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    710\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    712\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    714\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    715\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    716\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    717\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    718\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    719\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    720\u001b[0m     cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m    721\u001b[0m )\n\u001b[1;32m    722\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    724\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:353\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m+\u001b[39m causal_mask\n\u001b[1;32m    352\u001b[0m \u001b[39m# upcast attention to fp32\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49msoftmax(attn_weights, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\u001b[39m.\u001b[39mto(query_states\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    354\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(attn_weights, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_dropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m    355\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(attn_weights, value_states)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1860\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1858\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim)\n\u001b[1;32m   1859\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1860\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m   1861\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 14.57 GiB of which 22.75 MiB is free. Including non-PyTorch memory, this process has 14.54 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 349.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "def save_results(filename, results_dict, best_model):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump({'results_dict': results_dict, 'best_model': best_model}, f)\n",
    "\n",
    "def load_results(filename):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            return data['results_dict'], data['best_model']\n",
    "    else:\n",
    "        return {}, (None, float('-inf'))\n",
    "\n",
    "results_dict, best_model = load_results('mmlu_results.pkl')\n",
    "\n",
    "multipliers = [-1, 0, 1]\n",
    "experiment_counter = 0  # Counter to track number of experiments run\n",
    "save_interval = 10  # Save results every 10 experiments\n",
    "\n",
    "for corr_mult in multipliers:\n",
    "    for syco_mult in multipliers:\n",
    "        for truth_mult in multipliers:\n",
    "            for perm in itertools.permutations(layers):\n",
    "                # Check if this permutation has already been evaluated\n",
    "                key = (corr_mult, perm[1], syco_mult, perm[0], truth_mult, perm[2])\n",
    "                if key in results_dict:\n",
    "                    print(key)\n",
    "                    continue\n",
    "\n",
    "                # Retrieve preloaded steering vectors for the current permutation\n",
    "                sycophancy_steering_vector = sycophancy_vectors[perm[0]]\n",
    "                corrigibility_steering_vector = corrigibility_vectors[perm[1]]\n",
    "                truthfulness_steering_vector = truthfulness_vectors[perm[2]]\n",
    "\n",
    "                with corrigibility_steering_vector.apply(model, multiplier=corr_mult, min_token_index=0):\n",
    "                    with sycophancy_steering_vector.apply(model, multiplier=syco_mult, min_token_index=0):\n",
    "                        with truthfulness_steering_vector.apply(model, multiplier=truth_mult, min_token_index=0):\n",
    "                            start_time = time.time()\n",
    "                            # Evaluate model performance\n",
    "\n",
    "                            ''' Note: Commented out for mmlu evaluation only\n",
    "                            corrigibility_result = evaluate_model(model, tokenizer, corrigibility_mc_test_data, show_progress=False)\n",
    "                            sycophancy_result = evaluate_model(model, tokenizer, sycophancy_mc_test_data, show_progress=False)\n",
    "                            truthfulness_result = evaluate_model(model, tokenizer, truthfulness_mc_test_data, show_progress=False)\n",
    "                            \n",
    "                            overall_score = corrigibility_result + sycophancy_result + truthfulness_result\n",
    "                            '''\n",
    "                            mmlu_result = evaluate_model_mmlu(model, tokenizer, mmlu_mc_test_data, show_progress=False)\n",
    "                            overall_score = mmlu_result\n",
    "\n",
    "                            print(f\"MMLU Result: {mmlu_result}\")\n",
    "\n",
    "                            \n",
    "\n",
    "                            if overall_score > best_model[1]:\n",
    "                                best_model = (key, overall_score)\n",
    "                                print(f\"New Best Model Found — score of {overall_score}!\")\n",
    "                                print(f\"Details: Corrigibility Multiplier: {key[0]} at Layer {key[1]}, \"\n",
    "                                      f\"Sycophancy Multiplier: {key[2]} at Layer {key[3]}, \"\n",
    "                                      f\"Truthfulness Multiplier: {key[4]} at Layer {key[5]}\")\n",
    "                            '''\n",
    "                            # Store the results in the dictionary\n",
    "                            results_dict[key] = {\n",
    "                                'corrigibility_result': corrigibility_result,\n",
    "                                'sycophancy_result': sycophancy_result,\n",
    "                                'truthfulness_result': truthfulness_result\n",
    "                            }\n",
    "                            '''\n",
    "\n",
    "                            results_dict[key] = {\n",
    "                                'mmlu_result': mmlu_result\n",
    "                            }\n",
    "\n",
    "                            print(f\"Time to process one result: {time.time() - start_time}\")\n",
    "\n",
    "                            experiment_counter += 1\n",
    "                            if experiment_counter % save_interval == 0:\n",
    "                                save_results('mmlu_results.pkl', results_dict, best_model)\n",
    "                                print(\"Intermediate results saved.\")\n",
    "\n",
    "# Save the final results\n",
    "save_results('mmlu_results.pkl', results_dict, best_model)\n",
    "print(\"Final results saved.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    }
   ],
   "source": [
    "with open(\"mmlu_results.pkl\", \"rb\") as f:\n",
    "    results_dict = pickle.load(f)\n",
    "\n",
    "print(len(results_dict['results_dict'].keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
