{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file for reading in binary mode\n",
    "with open('results_dict.pkl', 'rb') as file:\n",
    "    # Load the object from the file\n",
    "    results = pickle.load(file)\n",
    "\n",
    "# Now you can use your_object as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(-1, 14, -1, 13, -1, 15): {'corrigibility_result': 0.2881906726499496, 'sycophancy_result': 0.5886924866918313, 'truthfulness_result': 0.6240551713800078}, (-1, 15, -1, 13, -1, 14): {'corrigibility_result': 0.2667849550084583, 'sycophancy_result': 0.5922001146052106, 'truthfulness_result': 0.597979469587372}, (-1, 13, -1, 14, -1, 15): {'corrigibility_result': 0.2853341162776644, 'sycophancy_result': 0.564145195843102, 'truthfulness_result': 0.6089925171758265}, (-1, 15, -1, 14, -1, 13): {'corrigibility_result': 0.2667849550084583, 'sycophancy_result': 0.5922001146052106, 'truthfulness_result': 0.597979469587372}, (-1, 13, -1, 15, -1, 14): {'corrigibility_result': 0.2853341162776644, 'sycophancy_result': 0.564145195843102, 'truthfulness_result': 0.6089925171758265}, (-1, 14, -1, 15, -1, 13): {'corrigibility_result': 0.2881906726499496, 'sycophancy_result': 0.5886924866918313, 'truthfulness_result': 0.6240551713800078}, (-1, 14, -1, 13, 0, 15): {'corrigibility_result': 0.21613492531325626, 'sycophancy_result': 0.645871041012708, 'truthfulness_result': 0.6209914930643549}, (-1, 15, -1, 13, 0, 14): {'corrigibility_result': 0.2550412105709693, 'sycophancy_result': 0.599665733388481, 'truthfulness_result': 0.5979953076122083}, (-1, 13, -1, 14, 0, 15): {'corrigibility_result': 0.26626186412116415, 'sycophancy_result': 0.5067109359061857, 'truthfulness_result': 0.5985726935225212}, (-1, 15, -1, 14, 0, 13): {'corrigibility_result': 0.25397527005160514, 'sycophancy_result': 0.6105735069737692, 'truthfulness_result': 0.6312407920870009}, (-1, 13, -1, 15, 0, 14): {'corrigibility_result': 0.2599777565189519, 'sycophancy_result': 0.5867957350352667, 'truthfulness_result': 0.587587263023907}, (-1, 14, -1, 15, 0, 13): {'corrigibility_result': 0.21652142873368527, 'sycophancy_result': 0.6197771235212441, 'truthfulness_result': 0.615626105125257}, (-1, 14, -1, 13, 1, 15): {'corrigibility_result': 0.21693831039536196, 'sycophancy_result': 0.5968817089566005, 'truthfulness_result': 0.6159195934011759}, (-1, 15, -1, 13, 1, 14): {'corrigibility_result': 0.24756872007052955, 'sycophancy_result': 0.5963129094676587, 'truthfulness_result': 0.609719340807192}, (-1, 13, -1, 14, 1, 15): {'corrigibility_result': 0.20626464544304704, 'sycophancy_result': 0.5570282606526844, 'truthfulness_result': 0.5738656690559606}, (-1, 15, -1, 14, 1, 13): {'corrigibility_result': 0.24262741206727031, 'sycophancy_result': 0.621958241246691, 'truthfulness_result': 0.6131162511196596}, (-1, 13, -1, 15, 1, 14): {'corrigibility_result': 0.2345107506046522, 'sycophancy_result': 0.47901675402289556, 'truthfulness_result': 0.5982629247727338}, (-1, 14, -1, 15, 1, 13): {'corrigibility_result': 0.208567370342797, 'sycophancy_result': 0.6617668914011158, 'truthfulness_result': 0.5992236134561358}, (-1, 14, 0, 13, -1, 15): {'corrigibility_result': 0.21652142873368527, 'sycophancy_result': 0.6197771235212441, 'truthfulness_result': 0.615626105125257}, (-1, 15, 0, 13, -1, 14): {'corrigibility_result': 0.25397527005160514, 'sycophancy_result': 0.6105735069737692, 'truthfulness_result': 0.6312407920870009}, (-1, 13, 0, 14, -1, 15): {'corrigibility_result': 0.2599777565189519, 'sycophancy_result': 0.5867957350352667, 'truthfulness_result': 0.587587263023907}, (-1, 15, 0, 14, -1, 13): {'corrigibility_result': 0.2550412105709693, 'sycophancy_result': 0.599665733388481, 'truthfulness_result': 0.5979953076122083}, (-1, 13, 0, 15, -1, 14): {'corrigibility_result': 0.26626186412116415, 'sycophancy_result': 0.5067109359061857, 'truthfulness_result': 0.5985726935225212}, (-1, 14, 0, 15, -1, 13): {'corrigibility_result': 0.21613492531325626, 'sycophancy_result': 0.645871041012708, 'truthfulness_result': 0.6209914930643549}, (-1, 14, 0, 13, 0, 15): {'corrigibility_result': 0.204513182209303, 'sycophancy_result': 0.6074034402744235, 'truthfulness_result': 0.618879388068801}, (-1, 15, 0, 13, 0, 14): {'corrigibility_result': 0.22503185535656742, 'sycophancy_result': 0.6182611804431479, 'truthfulness_result': 0.6051588883366903}, (-1, 13, 0, 14, 0, 15): {'corrigibility_result': 0.2240618705516492, 'sycophancy_result': 0.5185628729871974, 'truthfulness_result': 0.6053525633559027}, (-1, 15, 0, 14, 0, 13): {'corrigibility_result': 0.22503185535656742, 'sycophancy_result': 0.6182611804431479, 'truthfulness_result': 0.6051588883366903}, (-1, 13, 0, 15, 0, 14): {'corrigibility_result': 0.2240618705516492, 'sycophancy_result': 0.5185628729871974, 'truthfulness_result': 0.6053525633559027}, (-1, 14, 0, 15, 0, 13): {'corrigibility_result': 0.204513182209303, 'sycophancy_result': 0.6074034402744235, 'truthfulness_result': 0.618879388068801}, (-1, 14, 0, 13, 1, 15): {'corrigibility_result': 0.19612204155795143, 'sycophancy_result': 0.6557763812844426, 'truthfulness_result': 0.617475401900145}, (-1, 15, 0, 13, 1, 14): {'corrigibility_result': 0.23733946887428353, 'sycophancy_result': 0.6251100779322263, 'truthfulness_result': 0.6106511060952338}, (-1, 13, 0, 14, 1, 15): {'corrigibility_result': 0.22202780902070438, 'sycophancy_result': 0.5006479713992269, 'truthfulness_result': 0.5922455649695662}, (-1, 15, 0, 14, 1, 13): {'corrigibility_result': 0.24351482303769764, 'sycophancy_result': 0.6636825582671814, 'truthfulness_result': 0.6035161175347826}, (-1, 13, 0, 15, 1, 14): {'corrigibility_result': 0.23753710779664405, 'sycophancy_result': 0.4773078798211962, 'truthfulness_result': 0.590495326430474}, (-1, 14, 0, 15, 1, 13): {'corrigibility_result': 0.21372562786863777, 'sycophancy_result': 0.6353032310047123, 'truthfulness_result': 0.593331596751338}, (-1, 14, 1, 13, -1, 15): {'corrigibility_result': 0.208567370342797, 'sycophancy_result': 0.6617668914011158, 'truthfulness_result': 0.5992236134561358}, (-1, 15, 1, 13, -1, 14): {'corrigibility_result': 0.24262741206727031, 'sycophancy_result': 0.621958241246691, 'truthfulness_result': 0.6131162511196596}, (-1, 13, 1, 14, -1, 15): {'corrigibility_result': 0.2345107506046522, 'sycophancy_result': 0.47901675402289556, 'truthfulness_result': 0.5982629247727338}, (-1, 15, 1, 14, -1, 13): {'corrigibility_result': 0.24756872007052955, 'sycophancy_result': 0.5963129094676587, 'truthfulness_result': 0.609719340807192}, (-1, 13, 1, 15, -1, 14): {'corrigibility_result': 0.20626464544304704, 'sycophancy_result': 0.5570282606526844, 'truthfulness_result': 0.5738656690559606}, (-1, 14, 1, 15, -1, 13): {'corrigibility_result': 0.21693831039536196, 'sycophancy_result': 0.5968817089566005, 'truthfulness_result': 0.6159195934011759}, (-1, 14, 1, 13, 0, 15): {'corrigibility_result': 0.21372562786863777, 'sycophancy_result': 0.6353032310047123, 'truthfulness_result': 0.593331596751338}, (-1, 15, 1, 13, 0, 14): {'corrigibility_result': 0.24351482303769764, 'sycophancy_result': 0.6636825582671814, 'truthfulness_result': 0.6035161175347826}, (-1, 13, 1, 14, 0, 15): {'corrigibility_result': 0.23753710779664405, 'sycophancy_result': 0.4773078798211962, 'truthfulness_result': 0.590495326430474}, (-1, 15, 1, 14, 0, 13): {'corrigibility_result': 0.23733946887428353, 'sycophancy_result': 0.6251100779322263, 'truthfulness_result': 0.6106511060952338}, (-1, 13, 1, 15, 0, 14): {'corrigibility_result': 0.22202780902070438, 'sycophancy_result': 0.5006479713992269, 'truthfulness_result': 0.5922455649695662}, (-1, 14, 1, 15, 0, 13): {'corrigibility_result': 0.19612204155795143, 'sycophancy_result': 0.6557763812844426, 'truthfulness_result': 0.617475401900145}, (-1, 14, 1, 13, 1, 15): {'corrigibility_result': 0.25692000539226734, 'sycophancy_result': 0.6424170363364644, 'truthfulness_result': 0.5791734517152658}, (-1, 15, 1, 13, 1, 14): {'corrigibility_result': 0.2743337294399065, 'sycophancy_result': 0.6660700641226022, 'truthfulness_result': 0.6155135038704829}, (-1, 13, 1, 14, 1, 15): {'corrigibility_result': 0.2114086854559948, 'sycophancy_result': 0.5024865709651234, 'truthfulness_result': 0.5764493592127143}, (-1, 15, 1, 14, 1, 13): {'corrigibility_result': 0.2743337294399065, 'sycophancy_result': 0.6660700641226022, 'truthfulness_result': 0.6155135038704829}, (-1, 13, 1, 15, 1, 14): {'corrigibility_result': 0.2114086854559948, 'sycophancy_result': 0.5024865709651234, 'truthfulness_result': 0.5764493592127143}, (-1, 14, 1, 15, 1, 13): {'corrigibility_result': 0.25692000539226734, 'sycophancy_result': 0.6424170363364644, 'truthfulness_result': 0.5791734517152658}, (0, 14, -1, 13, -1, 15): {'corrigibility_result': 0.2096255883598064, 'sycophancy_result': 0.6650742082642658, 'truthfulness_result': 0.6069280702603048}, (0, 15, -1, 13, -1, 14): {'corrigibility_result': 0.18183422030274035, 'sycophancy_result': 0.61663861592767, 'truthfulness_result': 0.6292122108065158}, (0, 13, -1, 14, -1, 15): {'corrigibility_result': 0.21487354801533087, 'sycophancy_result': 0.6412034009937565, 'truthfulness_result': 0.626889442543402}, (0, 15, -1, 14, -1, 13): {'corrigibility_result': 0.18183422030274035, 'sycophancy_result': 0.61663861592767, 'truthfulness_result': 0.6292122108065158}, (0, 13, -1, 15, -1, 14): {'corrigibility_result': 0.21487354801533087, 'sycophancy_result': 0.6412034009937565, 'truthfulness_result': 0.626889442543402}, (0, 14, -1, 15, -1, 13): {'corrigibility_result': 0.2096255883598064, 'sycophancy_result': 0.6650742082642658, 'truthfulness_result': 0.6069280702603048}, (0, 14, -1, 13, 0, 15): {'corrigibility_result': 0.21353209746879426, 'sycophancy_result': 0.6461235648210933, 'truthfulness_result': 0.6114053771224625}, (0, 15, -1, 13, 0, 14): {'corrigibility_result': 0.21353209746879426, 'sycophancy_result': 0.6461235648210933, 'truthfulness_result': 0.6114053771224625}, (0, 13, -1, 14, 0, 15): {'corrigibility_result': 0.1991382697715634, 'sycophancy_result': 0.6849469294062206, 'truthfulness_result': 0.6128766752696004}, (0, 15, -1, 14, 0, 13): {'corrigibility_result': 0.1991382697715634, 'sycophancy_result': 0.6849469294062206, 'truthfulness_result': 0.6128766752696004}, (0, 13, -1, 15, 0, 14): {'corrigibility_result': 0.23634303011589555, 'sycophancy_result': 0.6398541729809161, 'truthfulness_result': 0.608222107342698}, (0, 14, -1, 15, 0, 13): {'corrigibility_result': 0.23634303011589555, 'sycophancy_result': 0.6398541729809161, 'truthfulness_result': 0.608222107342698}, (0, 14, -1, 13, 1, 15): {'corrigibility_result': 0.1965624116958756, 'sycophancy_result': 0.6698588054997692, 'truthfulness_result': 0.5959049616606663}, (0, 15, -1, 13, 1, 14): {'corrigibility_result': 0.18907659826197443, 'sycophancy_result': 0.6407528929705062, 'truthfulness_result': 0.5923514743648286}, (0, 13, -1, 14, 1, 15): {'corrigibility_result': 0.20231502077300195, 'sycophancy_result': 0.6738172624139712, 'truthfulness_result': 0.6118511774027428}, (0, 15, -1, 14, 1, 13): {'corrigibility_result': 0.24979743508831068, 'sycophancy_result': 0.6735720207367126, 'truthfulness_result': 0.6029868774371616}, (0, 13, -1, 15, 1, 14): {'corrigibility_result': 0.2256123502651113, 'sycophancy_result': 0.6742260716054034, 'truthfulness_result': 0.5964222538033991}, (0, 14, -1, 15, 1, 13): {'corrigibility_result': 0.231055552654937, 'sycophancy_result': 0.6976903004427464, 'truthfulness_result': 0.5828548920088076}, (0, 14, 0, 13, -1, 15): {'corrigibility_result': 0.23634303011589555, 'sycophancy_result': 0.6398541729809161, 'truthfulness_result': 0.608222107342698}, (0, 15, 0, 13, -1, 14): {'corrigibility_result': 0.1991382697715634, 'sycophancy_result': 0.6849469294062206, 'truthfulness_result': 0.6128766752696004}, (0, 13, 0, 14, -1, 15): {'corrigibility_result': 0.23634303011589555, 'sycophancy_result': 0.6398541729809161, 'truthfulness_result': 0.608222107342698}, (0, 15, 0, 14, -1, 13): {'corrigibility_result': 0.21353209746879426, 'sycophancy_result': 0.6461235648210933, 'truthfulness_result': 0.6114053771224625}, (0, 13, 0, 15, -1, 14): {'corrigibility_result': 0.1991382697715634, 'sycophancy_result': 0.6849469294062206, 'truthfulness_result': 0.6128766752696004}, (0, 14, 0, 15, -1, 13): {'corrigibility_result': 0.21353209746879426, 'sycophancy_result': 0.6461235648210933, 'truthfulness_result': 0.6114053771224625}, (0, 14, 0, 13, 0, 15): {'corrigibility_result': 0.22903570256968422, 'sycophancy_result': 0.6384982842821194, 'truthfulness_result': 0.6047271190406005}, (0, 15, 0, 13, 0, 14): {'corrigibility_result': 0.22903570256968422, 'sycophancy_result': 0.6384982842821194, 'truthfulness_result': 0.6047271190406005}, (0, 13, 0, 14, 0, 15): {'corrigibility_result': 0.22903570256968422, 'sycophancy_result': 0.6384982842821194, 'truthfulness_result': 0.6047271190406005}, (0, 15, 0, 14, 0, 13): {'corrigibility_result': 0.22903570256968422, 'sycophancy_result': 0.6384982842821194, 'truthfulness_result': 0.6047271190406005}, (0, 13, 0, 15, 0, 14): {'corrigibility_result': 0.22903570256968422, 'sycophancy_result': 0.6384982842821194, 'truthfulness_result': 0.6047271190406005}, (0, 14, 0, 15, 0, 13): {'corrigibility_result': 0.22903570256968422, 'sycophancy_result': 0.6384982842821194, 'truthfulness_result': 0.6047271190406005}, (0, 14, 0, 13, 1, 15): {'corrigibility_result': 0.23422998736007428, 'sycophancy_result': 0.6579117633541621, 'truthfulness_result': 0.5894111686601042}, (0, 15, 0, 13, 1, 14): {'corrigibility_result': 0.21608350822736047, 'sycophancy_result': 0.6683746133978492, 'truthfulness_result': 0.597078643630898}, (0, 13, 0, 14, 1, 15): {'corrigibility_result': 0.23422998736007428, 'sycophancy_result': 0.6579117633541621, 'truthfulness_result': 0.5894111686601042}, (0, 15, 0, 14, 1, 13): {'corrigibility_result': 0.28408524908504595, 'sycophancy_result': 0.6791180671476573, 'truthfulness_result': 0.5753807533322723}, (0, 13, 0, 15, 1, 14): {'corrigibility_result': 0.21608350822736047, 'sycophancy_result': 0.6683746133978492, 'truthfulness_result': 0.597078643630898}, (0, 14, 0, 15, 1, 13): {'corrigibility_result': 0.28408524908504595, 'sycophancy_result': 0.6791180671476573, 'truthfulness_result': 0.5753807533322723}, (0, 14, 1, 13, -1, 15): {'corrigibility_result': 0.231055552654937, 'sycophancy_result': 0.6976903004427464, 'truthfulness_result': 0.5828548920088076}, (0, 15, 1, 13, -1, 14): {'corrigibility_result': 0.24979743508831068, 'sycophancy_result': 0.6735720207367126, 'truthfulness_result': 0.6029868774371616}, (0, 13, 1, 14, -1, 15): {'corrigibility_result': 0.2256123502651113, 'sycophancy_result': 0.6742260716054034, 'truthfulness_result': 0.5964222538033991}, (0, 15, 1, 14, -1, 13): {'corrigibility_result': 0.18907659826197443, 'sycophancy_result': 0.6407528929705062, 'truthfulness_result': 0.5923514743648286}, (0, 13, 1, 15, -1, 14): {'corrigibility_result': 0.20231502077300195, 'sycophancy_result': 0.6738172624139712, 'truthfulness_result': 0.6118511774027428}, (0, 14, 1, 15, -1, 13): {'corrigibility_result': 0.1965624116958756, 'sycophancy_result': 0.6698588054997692, 'truthfulness_result': 0.5959049616606663}, (0, 14, 1, 13, 0, 15): {'corrigibility_result': 0.28408524908504595, 'sycophancy_result': 0.6791180671476573, 'truthfulness_result': 0.5753807533322723}, (0, 15, 1, 13, 0, 14): {'corrigibility_result': 0.28408524908504595, 'sycophancy_result': 0.6791180671476573, 'truthfulness_result': 0.5753807533322723}, (0, 13, 1, 14, 0, 15): {'corrigibility_result': 0.21608350822736047, 'sycophancy_result': 0.6683746133978492, 'truthfulness_result': 0.597078643630898}, (0, 15, 1, 14, 0, 13): {'corrigibility_result': 0.21608350822736047, 'sycophancy_result': 0.6683746133978492, 'truthfulness_result': 0.597078643630898}, (0, 13, 1, 15, 0, 14): {'corrigibility_result': 0.23422998736007428, 'sycophancy_result': 0.6579117633541621, 'truthfulness_result': 0.5894111686601042}, (0, 14, 1, 15, 0, 13): {'corrigibility_result': 0.23422998736007428, 'sycophancy_result': 0.6579117633541621, 'truthfulness_result': 0.5894111686601042}, (0, 14, 1, 13, 1, 15): {'corrigibility_result': 0.2780491568507074, 'sycophancy_result': 0.7008573809897963, 'truthfulness_result': 0.58233314643618}, (0, 15, 1, 13, 1, 14): {'corrigibility_result': 0.31204214597241, 'sycophancy_result': 0.6849800742046636, 'truthfulness_result': 0.5815367329937271}, (0, 13, 1, 14, 1, 15): {'corrigibility_result': 0.2495172598804762, 'sycophancy_result': 0.6725802697155768, 'truthfulness_result': 0.5954757612015048}, (0, 15, 1, 14, 1, 13): {'corrigibility_result': 0.31204214597241, 'sycophancy_result': 0.6849800742046636, 'truthfulness_result': 0.5815367329937271}, (0, 13, 1, 15, 1, 14): {'corrigibility_result': 0.2495172598804762, 'sycophancy_result': 0.6725802697155768, 'truthfulness_result': 0.5954757612015048}, (0, 14, 1, 15, 1, 13): {'corrigibility_result': 0.2780491568507074, 'sycophancy_result': 0.7008573809897963, 'truthfulness_result': 0.58233314643618}, (1, 14, -1, 13, -1, 15): {'corrigibility_result': 0.32491551300400867, 'sycophancy_result': 0.6566804476744222, 'truthfulness_result': 0.5685816859019648}, (1, 15, -1, 13, -1, 14): {'corrigibility_result': 0.18012886258523367, 'sycophancy_result': 0.6428789656312732, 'truthfulness_result': 0.5745172568146608}, (1, 13, -1, 14, -1, 15): {'corrigibility_result': 0.5600909698457499, 'sycophancy_result': 0.6101550640757967, 'truthfulness_result': 0.5449298429492877}, (1, 15, -1, 14, -1, 13): {'corrigibility_result': 0.18012886258523367, 'sycophancy_result': 0.6428789656312732, 'truthfulness_result': 0.5745172568146608}, (1, 13, -1, 15, -1, 14): {'corrigibility_result': 0.5600909698457499, 'sycophancy_result': 0.6101550640757967, 'truthfulness_result': 0.5449298429492877}, (1, 14, -1, 15, -1, 13): {'corrigibility_result': 0.32491551300400867, 'sycophancy_result': 0.6566804476744222, 'truthfulness_result': 0.5685816859019648}, (1, 14, -1, 13, 0, 15): {'corrigibility_result': 0.3008175182886511, 'sycophancy_result': 0.6474245134457536, 'truthfulness_result': 0.5803309436932391}, (1, 15, -1, 13, 0, 14): {'corrigibility_result': 0.23636362983697054, 'sycophancy_result': 0.6802921180050403, 'truthfulness_result': 0.572442869699536}, (1, 13, -1, 14, 0, 15): {'corrigibility_result': 0.5530092360170983, 'sycophancy_result': 0.6161359491210334, 'truthfulness_result': 0.5154563152589106}, (1, 15, -1, 14, 0, 13): {'corrigibility_result': 0.23055265707397424, 'sycophancy_result': 0.6561571935982536, 'truthfulness_result': 0.5738917376010251}, (1, 13, -1, 15, 0, 14): {'corrigibility_result': 0.5541241825738049, 'sycophancy_result': 0.5950618951989665, 'truthfulness_result': 0.5204097280218584}, (1, 14, -1, 15, 0, 13): {'corrigibility_result': 0.3247473147841328, 'sycophancy_result': 0.6682327704260793, 'truthfulness_result': 0.5451707201209893}, (1, 14, -1, 13, 1, 15): {'corrigibility_result': 0.3274201248152276, 'sycophancy_result': 0.6058581197662953, 'truthfulness_result': 0.5637945719561619}, (1, 15, -1, 13, 1, 14): {'corrigibility_result': 0.2821965787171716, 'sycophancy_result': 0.6380819356011149, 'truthfulness_result': 0.5819821335992595}, (1, 13, -1, 14, 1, 15): {'corrigibility_result': 0.56205207789095, 'sycophancy_result': 0.6348199888427201, 'truthfulness_result': 0.5242819573515795}, (1, 15, -1, 14, 1, 13): {'corrigibility_result': 0.331361616717582, 'sycophancy_result': 0.6246728333922573, 'truthfulness_result': 0.5621697977032634}, (1, 13, -1, 15, 1, 14): {'corrigibility_result': 0.5510308555263984, 'sycophancy_result': 0.6102819739640043, 'truthfulness_result': 0.5322972930345633}, (1, 14, -1, 15, 1, 13): {'corrigibility_result': 0.37614760808712044, 'sycophancy_result': 0.6611082044628074, 'truthfulness_result': 0.5433938006574297}, (1, 14, 0, 13, -1, 15): {'corrigibility_result': 0.3247473147841328, 'sycophancy_result': 0.6682327704260793, 'truthfulness_result': 0.5451707201209893}, (1, 15, 0, 13, -1, 14): {'corrigibility_result': 0.23055265707397424, 'sycophancy_result': 0.6561571935982536, 'truthfulness_result': 0.5738917376010251}, (1, 13, 0, 14, -1, 15): {'corrigibility_result': 0.5541241825738049, 'sycophancy_result': 0.5950618951989665, 'truthfulness_result': 0.5204097280218584}, (1, 15, 0, 14, -1, 13): {'corrigibility_result': 0.23636362983697054, 'sycophancy_result': 0.6802921180050403, 'truthfulness_result': 0.572442869699536}, (1, 13, 0, 15, -1, 14): {'corrigibility_result': 0.5530092360170983, 'sycophancy_result': 0.6161359491210334, 'truthfulness_result': 0.5154563152589106}, (1, 14, 0, 15, -1, 13): {'corrigibility_result': 0.3008175182886511, 'sycophancy_result': 0.6474245134457536, 'truthfulness_result': 0.5803309436932391}, (1, 14, 0, 13, 0, 15): {'corrigibility_result': 0.34173804411506475, 'sycophancy_result': 0.6784804025498404, 'truthfulness_result': 0.5428444344413882}, (1, 15, 0, 13, 0, 14): {'corrigibility_result': 0.2910565861300546, 'sycophancy_result': 0.6076196893136757, 'truthfulness_result': 0.5803346066506735}, (1, 13, 0, 14, 0, 15): {'corrigibility_result': 0.5709536054295753, 'sycophancy_result': 0.6316269482514911, 'truthfulness_result': 0.5319349234752544}, (1, 15, 0, 14, 0, 13): {'corrigibility_result': 0.2910565861300546, 'sycophancy_result': 0.6076196893136757, 'truthfulness_result': 0.5803346066506735}, (1, 13, 0, 15, 0, 14): {'corrigibility_result': 0.5709536054295753, 'sycophancy_result': 0.6316269482514911, 'truthfulness_result': 0.5319349234752544}, (1, 14, 0, 15, 0, 13): {'corrigibility_result': 0.34173804411506475, 'sycophancy_result': 0.6784804025498404, 'truthfulness_result': 0.5428444344413882}, (1, 14, 0, 13, 1, 15): {'corrigibility_result': 0.3550114699999215, 'sycophancy_result': 0.6792017847928835, 'truthfulness_result': 0.5425169900712268}, (1, 15, 0, 13, 1, 14): {'corrigibility_result': 0.36375107729192, 'sycophancy_result': 0.6420947750732456, 'truthfulness_result': 0.5653707740558138}, (1, 13, 0, 14, 1, 15): {'corrigibility_result': 0.5461356133317451, 'sycophancy_result': 0.621197196172941, 'truthfulness_result': 0.5214461139283995}, (1, 15, 0, 14, 1, 13): {'corrigibility_result': 0.37483755109835093, 'sycophancy_result': 0.6423627183460765, 'truthfulness_result': 0.5839300078756342}, (1, 13, 0, 15, 1, 14): {'corrigibility_result': 0.570564823211948, 'sycophancy_result': 0.6049647857653536, 'truthfulness_result': 0.542251438942808}, (1, 14, 0, 15, 1, 13): {'corrigibility_result': 0.4074430595196671, 'sycophancy_result': 0.6137372533990013, 'truthfulness_result': 0.5382767370373687}, (1, 14, 1, 13, -1, 15): {'corrigibility_result': 0.37614760808712044, 'sycophancy_result': 0.6611082044628074, 'truthfulness_result': 0.5433938006574297}, (1, 15, 1, 13, -1, 14): {'corrigibility_result': 0.331361616717582, 'sycophancy_result': 0.6246728333922573, 'truthfulness_result': 0.5621697977032634}, (1, 13, 1, 14, -1, 15): {'corrigibility_result': 0.5510308555263984, 'sycophancy_result': 0.6102819739640043, 'truthfulness_result': 0.5322972930345633}, (1, 15, 1, 14, -1, 13): {'corrigibility_result': 0.2821965787171716, 'sycophancy_result': 0.6380819356011149, 'truthfulness_result': 0.5819821335992595}, (1, 13, 1, 15, -1, 14): {'corrigibility_result': 0.56205207789095, 'sycophancy_result': 0.6348199888427201, 'truthfulness_result': 0.5242819573515795}, (1, 14, 1, 15, -1, 13): {'corrigibility_result': 0.3274201248152276, 'sycophancy_result': 0.6058581197662953, 'truthfulness_result': 0.5637945719561619}, (1, 14, 1, 13, 0, 15): {'corrigibility_result': 0.4074430595196671, 'sycophancy_result': 0.6137372533990013, 'truthfulness_result': 0.5382767370373687}, (1, 15, 1, 13, 0, 14): {'corrigibility_result': 0.37483755109835093, 'sycophancy_result': 0.6423627183460765, 'truthfulness_result': 0.5839300078756342}, (1, 13, 1, 14, 0, 15): {'corrigibility_result': 0.570564823211948, 'sycophancy_result': 0.6049647857653536, 'truthfulness_result': 0.542251438942808}, (1, 15, 1, 14, 0, 13): {'corrigibility_result': 0.36375107729192, 'sycophancy_result': 0.6420947750732456, 'truthfulness_result': 0.5653707740558138}, (1, 13, 1, 15, 0, 14): {'corrigibility_result': 0.5461356133317451, 'sycophancy_result': 0.621197196172941, 'truthfulness_result': 0.5214461139283995}, (1, 14, 1, 15, 0, 13): {'corrigibility_result': 0.3550114699999215, 'sycophancy_result': 0.6792017847928835, 'truthfulness_result': 0.5425169900712268}, (1, 14, 1, 13, 1, 15): {'corrigibility_result': 0.42984152472054526, 'sycophancy_result': 0.6809855493063117, 'truthfulness_result': 0.5617778030700857}, (1, 15, 1, 13, 1, 14): {'corrigibility_result': 0.4125420054768977, 'sycophancy_result': 0.6468960744810546, 'truthfulness_result': 0.5769597981006198}, (1, 13, 1, 14, 1, 15): {'corrigibility_result': 0.5671706483572281, 'sycophancy_result': 0.6849231713615338, 'truthfulness_result': 0.5357426790492801}, (1, 15, 1, 14, 1, 13): {'corrigibility_result': 0.4125420054768977, 'sycophancy_result': 0.6468960744810546, 'truthfulness_result': 0.5769597981006198}, (1, 13, 1, 15, 1, 14): {'corrigibility_result': 0.5671706483572281, 'sycophancy_result': 0.6849231713615338, 'truthfulness_result': 0.5357426790492801}, (1, 14, 1, 15, 1, 13): {'corrigibility_result': 0.42984152472054526, 'sycophancy_result': 0.6809855493063117, 'truthfulness_result': 0.5617778030700857}}\n"
     ]
    }
   ],
   "source": [
    "results_dict = results['results_dict']\n",
    "best_model = results['best_model']\n",
    "\n",
    "print(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum truthfulness_result: 0.6312407920870009\n",
      "Key (-1, 15, -1, 14, 0, 13)\n",
      "{'corrigibility_result': 0.22903570256968422, 'sycophancy_result': 0.6384982842821194, 'truthfulness_result': 0.6047271190406005}\n"
     ]
    }
   ],
   "source": [
    "max_truthfulness = float('-inf')\n",
    "max_truthfulness_key = None\n",
    "\n",
    "for key, value in results_dict.items():\n",
    "    # Update max_truthfulness if the current value is greater\n",
    "    if value['truthfulness_result'] > max_truthfulness:\n",
    "        max_truthfulness_key = key\n",
    "        max_truthfulness = value['truthfulness_result']\n",
    "\n",
    "# Output the maximum truthfulness_result\n",
    "print(\"Maximum truthfulness_result:\", max_truthfulness)\n",
    "print(\"Key\", max_truthfulness_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum corrigibility_result: 0.6312407920870009\n",
      "Key (1, 13, 0, 14, 0, 15)\n"
     ]
    }
   ],
   "source": [
    "max_corrigibility = float('-inf')\n",
    "max_corrigibility_key = None\n",
    "\n",
    "for key, value in results_dict.items():\n",
    "    if value['corrigibility_result'] > max_corrigibility:\n",
    "        max_corrigibility_key = key\n",
    "        max_corrigibility = value['corrigibility_result']\n",
    "\n",
    "print(\"Maximum corrigibility_result:\", max_truthfulness)\n",
    "print(\"Key\", max_corrigibility_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sycophancy_result: 0.7008573809897963\n",
      "Key (0, 14, 1, 13, 1, 15)\n"
     ]
    }
   ],
   "source": [
    "max_sycophancy = float('-inf')\n",
    "max_sycophancy_key = None\n",
    "\n",
    "for key, value in results_dict.items():\n",
    "    if value['sycophancy_result'] > max_sycophancy:\n",
    "        max_sycophancy_key = key\n",
    "        max_sycophancy = value['sycophancy_result']\n",
    "\n",
    "print(\"Maximum sycophancy_result:\", max_sycophancy)\n",
    "print(\"Key\", max_sycophancy_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corrigibility_result': 0.5709536054295753, 'sycophancy_result': 0.6316269482514911, 'truthfulness_result': 0.5319349234752544}\n",
      "{'corrigibility_result': 0.2780491568507074, 'sycophancy_result': 0.7008573809897963, 'truthfulness_result': 0.58233314643618}\n",
      "{'corrigibility_result': 0.25397527005160514, 'sycophancy_result': 0.6105735069737692, 'truthfulness_result': 0.6312407920870009}\n",
      "{'corrigibility_result': 0.22903570256968422, 'sycophancy_result': 0.6384982842821194, 'truthfulness_result': 0.6047271190406005}\n",
      "{'corrigibility_result': 0.5671706483572281, 'sycophancy_result': 0.6849231713615338, 'truthfulness_result': 0.5357426790492801}\n"
     ]
    }
   ],
   "source": [
    "baseline_key = (0,13,0,14,0,15)\n",
    "best_model_key = (1,13,1,14,1,15)\n",
    "\n",
    "print(results_dict[max_corrigibility_key])\n",
    "print(results_dict[max_sycophancy_key])\n",
    "print(results_dict[max_truthfulness_key])\n",
    "print(results_dict[baseline_key])\n",
    "print(results_dict[best_model_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys where all values are above baseline: [(-1, 15, 1, 13, 1, 14), (-1, 15, 1, 14, 1, 13), (0, 13, -1, 15, 0, 14), (0, 14, -1, 15, 0, 13), (0, 14, 0, 13, -1, 15), (0, 13, 0, 14, -1, 15)]\n"
     ]
    }
   ],
   "source": [
    "# Baseline values\n",
    "baseline_key = (0,13,0,14,0,15)\n",
    "baseline_values = results_dict[baseline_key]\n",
    "\n",
    "# List to store keys where all values are above baseline\n",
    "keys_above_baseline = []\n",
    "\n",
    "# Iterate through each key-value pair in the dictionary\n",
    "for key, value in results_dict.items():\n",
    "    # Flag to check if all values are above baseline\n",
    "    all_above_baseline = True\n",
    "    # Iterate through each metric\n",
    "    for metric, baseline in baseline_values.items():\n",
    "        # Check if the value for the current metric is above baseline\n",
    "        if value[metric] <= baseline:\n",
    "            all_above_baseline = False\n",
    "            break\n",
    "    # If all values are above baseline, append the key to the list\n",
    "    if all_above_baseline:\n",
    "        keys_above_baseline.append(key)\n",
    "\n",
    "# Output the keys where all values are above baseline\n",
    "print(\"Keys where all values are above baseline:\", keys_above_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1, 15, 1, 13, 1, 14) {'corrigibility_result': 0.2743337294399065, 'sycophancy_result': 0.6660700641226022, 'truthfulness_result': 0.6155135038704829}\n",
      "(-1, 15, 1, 14, 1, 13) {'corrigibility_result': 0.2743337294399065, 'sycophancy_result': 0.6660700641226022, 'truthfulness_result': 0.6155135038704829}\n",
      "(0, 13, -1, 15, 0, 14) {'corrigibility_result': 0.23634303011589555, 'sycophancy_result': 0.6398541729809161, 'truthfulness_result': 0.608222107342698}\n",
      "(0, 14, -1, 15, 0, 13) {'corrigibility_result': 0.23634303011589555, 'sycophancy_result': 0.6398541729809161, 'truthfulness_result': 0.608222107342698}\n",
      "(0, 14, 0, 13, -1, 15) {'corrigibility_result': 0.23634303011589555, 'sycophancy_result': 0.6398541729809161, 'truthfulness_result': 0.608222107342698}\n",
      "(0, 13, 0, 14, -1, 15) {'corrigibility_result': 0.23634303011589555, 'sycophancy_result': 0.6398541729809161, 'truthfulness_result': 0.608222107342698}\n",
      "(0, 13, 0, 14, 0, 15) {'corrigibility_result': 0.22903570256968422, 'sycophancy_result': 0.6384982842821194, 'truthfulness_result': 0.6047271190406005}\n"
     ]
    }
   ],
   "source": [
    "for key in keys_above_baseline:\n",
    "    print(key, results_dict[key])\n",
    "\n",
    "print((0, 13, 0, 14, 0, 15), results_dict[baseline_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = (0, 13, 0, 14, 0, 15)\n",
    "best_sum = (1, 13, 1, 14, 1, 15)\n",
    "best_corrigibility = (1, 13, 0, 14, 0, 15)\n",
    "best_sycophancy = (0, 14, 1, 13, 1, 15)\n",
    "best_truthfulness = (-1, 15, -1, 14, 0, 13)\n",
    "best_above_1 = (-1, 15, 1, 13, 1, 14)\n",
    "best_above_2 = (-1, 15, 1, 14, 1, 13)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
